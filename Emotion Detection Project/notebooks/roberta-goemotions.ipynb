{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3004895,"sourceType":"datasetVersion","datasetId":1840804}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training RoBERTa on GoEmotions","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Ignore the harmless registration warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nprint(\"✅ Libraries imported successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:35:55.285821Z","iopub.execute_input":"2025-08-02T04:35:55.286447Z","iopub.status.idle":"2025-08-02T04:36:04.867937Z","shell.execute_reply.started":"2025-08-02T04:35:55.286413Z","shell.execute_reply":"2025-08-02T04:36:04.867214Z"}},"outputs":[{"name":"stdout","text":"✅ Libraries imported successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Data Load","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/goemotions/GoEmotions.csv')\nprint(\"\\n✅ Dataset loaded successfully.\")\n\n\n# Define emotion labels\nemotion_cols = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', \n                'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', \n                'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', \n                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:04.869073Z","iopub.execute_input":"2025-08-02T04:36:04.869523Z","iopub.status.idle":"2025-08-02T04:36:06.420484Z","shell.execute_reply.started":"2025-08-02T04:36:04.869504Z","shell.execute_reply":"2025-08-02T04:36:06.419717Z"}},"outputs":[{"name":"stdout","text":"\n✅ Dataset loaded successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data Labeling","metadata":{}},{"cell_type":"code","source":"# Prepare labels for the model\ndf['labels'] = df[emotion_cols].values.tolist()\ndf_model = df[['text', 'labels']].copy()\n\n# Using 50k samples for a robust comparison\ndf_sample = df_model.sample(n=50000, random_state=42)\ntrain_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42)\n\nprint(\"\\n✅ Data prepared:\")\nprint(f\" - Training set size: {len(train_df)}\")\nprint(f\" - Test set size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:06.421171Z","iopub.execute_input":"2025-08-02T04:36:06.421430Z","iopub.status.idle":"2025-08-02T04:36:06.985947Z","shell.execute_reply.started":"2025-08-02T04:36:06.421403Z","shell.execute_reply":"2025-08-02T04:36:06.985237Z"}},"outputs":[{"name":"stdout","text":"\n✅ Data prepared:\n - Training set size: 40000\n - Test set size: 10000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Model and Tokenization","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nUsing device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:06.987543Z","iopub.execute_input":"2025-08-02T04:36:06.987789Z","iopub.status.idle":"2025-08-02T04:36:07.075471Z","shell.execute_reply.started":"2025-08-02T04:36:06.987771Z","shell.execute_reply":"2025-08-02T04:36:07.074652Z"}},"outputs":[{"name":"stdout","text":"\nUsing device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"MODEL_NAME = 'roberta-base'\nprint(f\"\\n🔥 Loading model: {MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(emotion_cols),\n    problem_type=\"multi_label_classification\"\n)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:07.076197Z","iopub.execute_input":"2025-08-02T04:36:07.076465Z","iopub.status.idle":"2025-08-02T04:36:29.245005Z","shell.execute_reply.started":"2025-08-02T04:36:07.076440Z","shell.execute_reply":"2025-08-02T04:36:29.244343Z"}},"outputs":[{"name":"stdout","text":"\n🔥 Loading model: roberta-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef7314b76da46eea7f5181636fa74e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3ec3de2b114780a4a80244eba8fedd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89208ee0091b4f35b3cf04beff6b0b89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f954933318c41f7abde1eecbbe0b6e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf222f11972947d88c1d65624cb054ac"}},"metadata":{}},{"name":"stderr","text":"2025-08-02 04:36:14.737401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754109374.983337      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754109375.054112      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a32f294c935641cc9cf2e6c92c6ab9f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### Weights for Class Imbalance","metadata":{}},{"cell_type":"code","source":"print(\"\\n⚖️ Calculating weights for handling class imbalance...\")\n\n# Recreate the emotion columns from the 'labels' list to perform the sum\ntemp_labels_df = pd.DataFrame(train_df['labels'].tolist(), columns=emotion_cols)\npositive_counts = temp_labels_df.sum()\n\nnegative_counts = len(train_df) - positive_counts\nclass_weights = negative_counts / positive_counts\n# Now we can safely move the tensor to the device\npos_weight_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\nprint(\"✅ Weights calculated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:29.245827Z","iopub.execute_input":"2025-08-02T04:36:29.246256Z","iopub.status.idle":"2025-08-02T04:36:29.479940Z","shell.execute_reply.started":"2025-08-02T04:36:29.246240Z","shell.execute_reply":"2025-08-02T04:36:29.479350Z"}},"outputs":[{"name":"stdout","text":"\n⚖️ Calculating weights for handling class imbalance...\n✅ Weights calculated.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"MAX_LEN = 128\nclass GoEmotionsDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer, self.text, self.labels, self.max_len = tokenizer, dataframe.text.values, dataframe.labels.values, max_len\n    def __len__(self): return len(self.text)\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, max_length=self.max_len, padding='max_length', return_token_type_ids=True, truncation=True)\n        return {'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long), 'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long), 'labels': torch.tensor(self.labels[index], dtype=torch.float)}\n\ntrain_dataset = GoEmotionsDataset(train_df, tokenizer, MAX_LEN)\ntest_dataset = GoEmotionsDataset(test_df, tokenizer, MAX_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:29.480661Z","iopub.execute_input":"2025-08-02T04:36:29.480955Z","iopub.status.idle":"2025-08-02T04:36:29.487011Z","shell.execute_reply.started":"2025-08-02T04:36:29.480936Z","shell.execute_reply":"2025-08-02T04:36:29.486460Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"BATCH_SIZE = 32 \ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(\"\\n✅ PyTorch Datasets and DataLoaders created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:29.487897Z","iopub.execute_input":"2025-08-02T04:36:29.488597Z","iopub.status.idle":"2025-08-02T04:36:29.502039Z","shell.execute_reply.started":"2025-08-02T04:36:29.488579Z","shell.execute_reply":"2025-08-02T04:36:29.501262Z"}},"outputs":[{"name":"stdout","text":"\n✅ PyTorch Datasets and DataLoaders created.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Model Setup","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\nprint(\"\\n✅ Model, optimizer, and weighted loss function are set up.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:29.502720Z","iopub.execute_input":"2025-08-02T04:36:29.502884Z","iopub.status.idle":"2025-08-02T04:36:29.516189Z","shell.execute_reply.started":"2025-08-02T04:36:29.502870Z","shell.execute_reply":"2025-08-02T04:36:29.515710Z"}},"outputs":[{"name":"stdout","text":"\n✅ Model, optimizer, and weighted loss function are set up.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Manual Training","metadata":{}},{"cell_type":"code","source":"NUM_EPOCHS = 4 # Using 4 epochs as planned\nprint(\"\\n🚀 Starting model fine-tuning with weighted loss...\")\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=False)\n    for batch in progress_bar:\n        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = criterion(outputs.logits, labels)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        progress_bar.set_postfix({'training_loss': f'{loss.item():.3f}'})\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"\\nEpoch {epoch + 1} | Average Training Loss: {avg_train_loss:.4f}\")\n\nprint(\"\\n✅ Model training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T04:36:29.517900Z","iopub.execute_input":"2025-08-02T04:36:29.518269Z","iopub.status.idle":"2025-08-02T05:32:18.382835Z","shell.execute_reply.started":"2025-08-02T04:36:29.518243Z","shell.execute_reply":"2025-08-02T05:32:18.382161Z"}},"outputs":[{"name":"stdout","text":"\n🚀 Starting model fine-tuning with weighted loss...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/4:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1 | Average Training Loss: 1.0073\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/4:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2 | Average Training Loss: 0.8368\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/4:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3 | Average Training Loss: 0.7444\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/4:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4 | Average Training Loss: 0.6670\n\n✅ Model training complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"\\n📊 Evaluating the model...\")\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions = (torch.sigmoid(logits) > 0.6).int().cpu().numpy()\n        all_preds.extend(predictions)\n        all_labels.extend(labels.cpu().numpy())\nprint(\"\\nFinal Classification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=emotion_cols, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:39:31.860934Z","iopub.execute_input":"2025-08-02T05:39:31.861202Z","iopub.status.idle":"2025-08-02T05:40:37.929551Z","shell.execute_reply.started":"2025-08-02T05:39:31.861182Z","shell.execute_reply":"2025-08-02T05:40:37.928785Z"}},"outputs":[{"name":"stdout","text":"\n📊 Evaluating the model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d4b2a0d5cd4766b2f357e88a8e261e"}},"metadata":{}},{"name":"stdout","text":"\nFinal Classification Report:\n                precision    recall  f1-score   support\n\n    admiration       0.34      0.79      0.48       821\n     amusement       0.38      0.78      0.51       447\n         anger       0.19      0.70      0.30       405\n     annoyance       0.18      0.66      0.28       671\n      approval       0.18      0.52      0.26       804\n        caring       0.15      0.67      0.25       269\n     confusion       0.16      0.82      0.27       341\n     curiosity       0.24      0.84      0.37       451\n        desire       0.10      0.64      0.17       188\ndisappointment       0.10      0.70      0.18       423\n   disapproval       0.17      0.69      0.28       563\n       disgust       0.11      0.73      0.19       244\n embarrassment       0.05      0.62      0.09       112\n    excitement       0.12      0.60      0.20       264\n          fear       0.17      0.67      0.27       149\n     gratitude       0.51      0.89      0.64       569\n         grief       0.04      0.59      0.08        27\n           joy       0.16      0.76      0.27       366\n          love       0.42      0.88      0.57       416\n   nervousness       0.05      0.59      0.10        95\n      optimism       0.15      0.69      0.24       404\n         pride       0.02      0.40      0.03        45\n   realization       0.09      0.51      0.16       393\n        relief       0.07      0.66      0.13        61\n       remorse       0.17      0.73      0.28       133\n       sadness       0.16      0.75      0.27       327\n      surprise       0.15      0.71      0.25       255\n       neutral       0.48      0.47      0.48      2578\n\n     micro avg       0.19      0.66      0.29     11821\n     macro avg       0.18      0.68      0.27     11821\n  weighted avg       0.27      0.66      0.35     11821\n   samples avg       0.23      0.64      0.32     11821\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"np.save('y_test_roberta.npy', all_labels) \nnp.save('y_pred_roberta.npy', all_preds)\nprint(\"\\n✅ Saved RoBERTa results to 'y_pred_roberta.npy'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:53:37.732725Z","iopub.execute_input":"2025-08-02T05:53:37.733498Z","iopub.status.idle":"2025-08-02T05:53:37.752072Z","shell.execute_reply.started":"2025-08-02T05:53:37.733475Z","shell.execute_reply":"2025-08-02T05:53:37.751498Z"}},"outputs":[{"name":"stdout","text":"\n✅ Saved RoBERTa results to 'y_pred_roberta.npy'\n","output_type":"stream"}],"execution_count":14}]}